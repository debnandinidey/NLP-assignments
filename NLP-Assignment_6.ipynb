{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "25f585f7",
   "metadata": {},
   "source": [
    "#### 1. What are Vanilla autoencoders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "056d2dda",
   "metadata": {},
   "source": [
    "Vanilla autoencoders, also known as basic autoencoders or traditional autoencoders, are a type of neural network architecture used for unsupervised learning and data compression. They are designed to learn an efficient representation (encoding) of the input data by training an encoder-decoder model.The architecture of a vanilla autoencoder consists of two main parts: an encoder and a decoder. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b83d81",
   "metadata": {},
   "source": [
    "#### 2. What are Sparse autoencoders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86bbf3fc",
   "metadata": {},
   "source": [
    "Sparse autoencoders are a variation of autoencoders that are specifically designed to learn sparse representations of the input data. In contrast to vanilla autoencoders, which aim to learn a compact representation of the data, sparse autoencoders encourage the learned representations to be sparse, meaning that only a small number of neurons in the hidden layer are activated for a given input."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b0564ae",
   "metadata": {},
   "source": [
    "#### 3. What are Denoising autoencoders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00914926",
   "metadata": {},
   "source": [
    "Denoising autoencoders are a type of autoencoder that are specifically designed to reconstruct clean and noise-free input data from corrupted or noisy versions of the data. They are trained to learn a compressed representation of the input data while being robust to noise and variations.The training process of denoising autoencoders involves corrupting the input data by adding noise or introducing other types of perturbations. The autoencoder is then trained to reconstruct the original, clean data from the corrupted input. By learning to recover the clean data from the noisy input, the denoising autoencoder implicitly learns useful features and patterns in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fce8eec",
   "metadata": {},
   "source": [
    "#### 4. What are Convolutional autoencoders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a077f931",
   "metadata": {},
   "source": [
    "Convolutional autoencoders are a type of autoencoder that utilize convolutional layers in their architecture, making them well-suited for processing and reconstructing structured data such as images. They leverage the inherent spatial relationships and local patterns present in the data by using convolutional filters.The basic structure of a convolutional autoencoder consists of an encoder and a decoder. The encoder part consists of several convolutional layers followed by pooling or downsampling layers, which progressively reduce the spatial dimensions of the input data while capturing its relevant features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12660fc9",
   "metadata": {},
   "source": [
    "#### 5. What are Stacked autoencoders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b51a587",
   "metadata": {},
   "source": [
    "Stacked autoencoders, also known as deep autoencoders, are a type of autoencoder architecture that consists of multiple layers of encoder and decoder pairs stacked on top of each other. Each layer serves as the input for the subsequent layer, forming a hierarchical structure. Stacked autoencoders are designed to learn increasingly abstract and complex representations of the input data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4372f07f",
   "metadata": {},
   "source": [
    "#### 6. Explain how to generate sentences using LSTM autoencoders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6758fe23",
   "metadata": {},
   "source": [
    "To generate sentences using LSTM autoencoders:\n",
    "\n",
    "Prepare and preprocess the training data by cleaning and tokenizing the sentences.\n",
    "\n",
    "Split the data into training and validation sets.\n",
    "\n",
    "Build an LSTM autoencoder model with an encoder and decoder.\n",
    "\n",
    "Train the model to minimize the reconstruction error between input and output sentences.\n",
    "\n",
    "Use the trained model to generate sentences by providing a seed sentence to the encoder and iteratively decoding the output.\n",
    "\n",
    "Introduce randomness by sampling from the word distribution at each step.\n",
    "\n",
    "Evaluate the generated sentences and refine the model if needed.\n",
    "\n",
    "Continue training and refining until desired results are achieved."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a84289a5",
   "metadata": {},
   "source": [
    "#### 7. Explain Extractive summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd7d98a2",
   "metadata": {},
   "source": [
    "Extractive summarization is a text summarization technique that involves selecting and combining important sentences or phrases from the original text to create a summary. It aims to extract the most relevant and informative content while preserving the original context."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c77e6d9",
   "metadata": {},
   "source": [
    "#### 8. Explain Abstractive summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5050f4fe",
   "metadata": {},
   "source": [
    "Abstractive summarization is a text summarization technique that aims to generate a summary by understanding the content of the original text and producing concise and coherent sentences that may not exist in the original text. Unlike extractive summarization, which selects and combines existing sentences, abstractive summarization involves language generation to create a summary that captures the main ideas of the text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbbe9e24",
   "metadata": {},
   "source": [
    "#### 9. Explain Beam search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3645db6e",
   "metadata": {},
   "source": [
    "Beam search is a search algorithm commonly used in natural language processing tasks, including machine translation and text generation, to find the most likely sequence of words given a model's predictions. It is particularly useful in scenarios where the output sequence is generated word by word and the goal is to find the sequence with the highest probability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da71e0f2",
   "metadata": {},
   "source": [
    "#### 10. Explain Length normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edc02877",
   "metadata": {},
   "source": [
    "Length normalization is a technique used in natural language processing, particularly in tasks like machine translation or text generation, to adjust the scores or probabilities of generated sequences based on their lengths. The purpose of length normalization is to prevent longer sequences from being penalized compared to shorter sequences and to promote fairness in evaluating different sequence lengths."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f89d3f7e",
   "metadata": {},
   "source": [
    "#### 11. Explain Coverage normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "537312b2",
   "metadata": {},
   "source": [
    "Coverage normalization is a technique used in sequence-to-sequence models, particularly in tasks like text summarization, to address the problem of repetitiveness in the generated sequences. It aims to ensure that important information in the source sequence is adequately covered and prevents the model from generating redundant or repetitive words or phrases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ae96b82",
   "metadata": {},
   "source": [
    "#### 12. Explain ROUGE metric evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fad9c527",
   "metadata": {},
   "source": [
    "ROUGE (Recall-Oriented Understudy for Gisting Evaluation) is a set of evaluation metrics commonly used to assess the quality of automatic summaries generated by text summarization systems. ROUGE measures the overlap between the generated summary and one or more reference summaries by considering various aspects such as word matching, n-gram matching, and sequence alignment.There are several versions of ROUGE metrics, including ROUGE-N, ROUGE-L, ROUGE-S, ROUGE-SU, and more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25341d9f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
