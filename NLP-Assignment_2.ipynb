{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "85e2d1d5",
   "metadata": {},
   "source": [
    "1. What are Corpora?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ae38c11",
   "metadata": {},
   "source": [
    "ans: Corpora, in the context of natural language processing (NLP), refer to large and structured collections of text documents. A corpus (plural: corpora) serves as a representative sample of a particular language or domain and is used as a basis for linguistic analysis, language modeling, and training various NLP models.Corpora can be composed of various types of text sources, including books, articles, web pages, social media posts, scientific papers, and more."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b1d41f",
   "metadata": {},
   "source": [
    "2. What are Tokens?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce0096df",
   "metadata": {},
   "source": [
    "ans: In the context of natural language processing (NLP), tokens refer to individual units or elements that make up a piece of text. A token can be as small as a single character or as large as a word, phrase, or even a sentence, depending on the chosen level of granularity.\n",
    "\n",
    "Tokenization is the process of breaking down a text into its constituent tokens. It involves segmenting the text into meaningful units, such as words or subwords, while excluding irrelevant elements like punctuation marks and whitespace. The resulting tokens serve as the basic building blocks for various NLP tasks and analyses."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e60424bf",
   "metadata": {},
   "source": [
    "3. What are Unigrams, Bigrams, Trigrams?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce1c3aa",
   "metadata": {},
   "source": [
    "Unigrams, bigrams, and trigrams are different types of n-grams, which are contiguous sequences of n items (typically words) extracted from a piece of text. N-grams are commonly used in natural language processing and text mining tasks to capture and analyze patterns and relationships within a text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb4f6c4",
   "metadata": {},
   "source": [
    "4. How to generate n-grams from text?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24765bad",
   "metadata": {},
   "source": [
    "Tokenize the Text: Start by splitting the text into individual tokens or words. we can use a tokenizer library or function to achieve this.\n",
    "\n",
    "Create n-grams: Iterate through the tokens and form n-grams based on the desired value of n. For example, to create bigrams (n = 2), we can take each consecutive pair of tokens and combine them into a single n-gram. Similarly, for trigrams (n = 3), we can combine three consecutive tokens into a single n-gram.\n",
    "\n",
    "Handle Boundary Cases: Depending on the desired n-gram length, we need to handle the boundary cases. For example, if we are generating trigrams, we need to consider the first two tokens as the initial prefix and the last two tokens as the final suffix."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8472c51",
   "metadata": {},
   "source": [
    "5. Explain Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c54da0d8",
   "metadata": {},
   "source": [
    "ans: Lemmatization is the process of reducing words to their base or root form, known as a lemma. The resulting lemma represents the canonical or dictionary form of the word. It aims to normalize words by eliminating variations due to inflection or conjugation.The goal of lemmatization is to group together different inflected forms of a word so that they can be analyzed as a single item. For example, the lemma of the words \"running,\" \"runs,\" and \"ran\" is \"run.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70624532",
   "metadata": {},
   "source": [
    "6. Explain Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6780ca5",
   "metadata": {},
   "source": [
    "ans: Stemming is a text normalization technique used to reduce words to their base or root form, known as a stem. The stemming process involves removing prefixes, suffixes, and other affixes from words, with the aim of reducing words to their common linguistic root.The goal of stemming is to transform words so that different variations of the same word are treated as the same word. For example, the stem of the words \"running,\" \"runs,\" and \"ran\" is \"run.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17f5c6f7",
   "metadata": {},
   "source": [
    "7. Explain Part-of-speech (POS) tagging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc8137db",
   "metadata": {},
   "source": [
    "ans: Part-of-speech (POS) tagging, also known as grammatical tagging, is the process of assigning grammatical labels or tags to each word in a given text, indicating the word's syntactic category or part of speech. POS tags provide information about the role and function of words in a sentence, such as whether a word is a noun, verb, adjective, adverb, etc.\n",
    "\n",
    "POS tagging is an important step in natural language processing and computational linguistics as it helps in understanding the grammatical structure and meaning of text. By assigning appropriate tags to words, it enables language models and algorithms to better analyze and interpret the text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28650186",
   "metadata": {},
   "source": [
    "8. Explain Chunking or shallow parsing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "391c6508",
   "metadata": {},
   "source": [
    "ans: Chunking, also known as shallow parsing, is a natural language processing technique that involves grouping words together into meaningful chunks based on their syntactic structure. The goal of chunking is to identify and extract phrases or noun phrases that convey specific information or have specific grammatical roles within a sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35346967",
   "metadata": {},
   "source": [
    "9. Explain Noun Phrase (NP) chunking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "278d5617",
   "metadata": {},
   "source": [
    "ans: Noun Phrase (NP) chunking is a specific type of chunking that focuses on identifying and extracting noun phrases from text. A noun phrase is a syntactic structure that includes a noun and other words that modify or describe that noun. It typically consists of a noun and any accompanying determiners, adjectives, and other modifiers.\n",
    "\n",
    "The process of NP chunking involves identifying and grouping words together that form a noun phrase. This is usually done after part-of-speech tagging, where each word in a sentence is assigned a part-of-speech tag."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5471b90",
   "metadata": {},
   "source": [
    "10. Explain Named Entity Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b5170f",
   "metadata": {},
   "source": [
    "Named Entity Recognition (NER) is a natural language processing task that involves identifying and classifying named entities in text. Named entities are specific words or phrases that refer to real-world entities such as persons, organizations, locations, dates, and more.\n",
    "\n",
    "The goal of NER is to automatically identify and classify these named entities into predefined categories. For example, in the sentence \"Apple Inc. is headquartered in Cupertino,\" NER would identify \"Apple Inc.\" as an organization and \"Cupertino\" as a location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf07e4b3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
