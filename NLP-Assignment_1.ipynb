{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "25b933a1",
   "metadata": {},
   "source": [
    "1. Explain One-Hot Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "268fcadf",
   "metadata": {},
   "source": [
    "ans:One-hot encoding is a technique used to represent categorical data in a numerical format that machine learning models can understand. It converts categorical variables into binary vectors where each category is represented by a binary value (0 or 1)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f275ea",
   "metadata": {},
   "source": [
    "2. Explain Bag of Words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1287560",
   "metadata": {},
   "source": [
    "ans: Bag of Words (BoW) is a text representation technique used in natural language processing and information retrieval tasks. It treats a document as an unordered collection or \"bag\" of words, disregarding grammar and word order. BoW represents a document by counting the frequency of each word present in the document and creating a numerical vector that represents the document's content."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bb7ab2d",
   "metadata": {},
   "source": [
    "3. Explain Bag of N-Grams"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e2a64d",
   "metadata": {},
   "source": [
    "ans:Bag of N-Grams is an extension of the Bag of Words (BoW) representation in natural language processing. Instead of considering individual words as features, Bag of N-Grams considers sequences of N consecutive words, called N-grams, as the atomic units of representation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "757add33",
   "metadata": {},
   "source": [
    " 4. Explain TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ab052f5",
   "metadata": {},
   "source": [
    "ans: TF-IDF (Term Frequency-Inverse Document Frequency) is a numerical representation technique commonly used in natural language processing to quantify the importance of a term in a document within a collection of documents. It takes into account both the term frequency (TF) and the inverse document frequency (IDF).\n",
    "\n",
    "The TF component measures the frequency of a term within a specific document. It is calculated as the ratio of the number of occurrences of a term in a document to the total number of terms in that document. TF provides a measure of how relevant a term is within a specific document."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a65433ea",
   "metadata": {},
   "source": [
    "5. What is OOV problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cdd9fb7",
   "metadata": {},
   "source": [
    "ans: The OOV (Out-of-Vocabulary) problem refers to the situation where a word or term is encountered in a text that is not present in the vocabulary or training data of a natural language processing or machine learning model. It is a common challenge in language processing tasks where the model is expected to handle unseen or unknown words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e3aea5",
   "metadata": {},
   "source": [
    "6. What are word embeddings?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0657ffd9",
   "metadata": {},
   "source": [
    "ans: Word embeddings are dense vector representations of words that capture semantic and contextual information about words in a language. They are numerical representations that map words to points in a multi-dimensional space, where similar words are closer to each other based on their semantic meaning. Word embeddings are typically learned through unsupervised learning techniques, such as neural network-based models like Word2Vec, GloVe, or FastText. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c599b2f",
   "metadata": {},
   "source": [
    "7. Explain Continuous bag of words (CBOW)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b680496e",
   "metadata": {},
   "source": [
    "ans: Continuous Bag of Words (CBOW) is a model used for generating word embeddings in natural language processing. It is a type of neural network architecture that aims to predict a target word based on its surrounding context words. CBOW treats the context words as inputs and the target word as the output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "817e10dd",
   "metadata": {},
   "source": [
    "8. Explain SkipGram"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4606df32",
   "metadata": {},
   "source": [
    "ans: Skip-gram is a model used for generating word embeddings in natural language processing. It is the inverse of the Continuous Bag of Words (CBOW) model. While CBOW predicts the target word based on its surrounding context words, Skip-gram predicts the surrounding context words given a target word.\n",
    "\n",
    "In the Skip-gram model, the target word is represented as a one-hot vector and is passed through an input layer. The input layer is connected to a hidden layer, which serves as the word embedding layer. The hidden layer aims to encode the target word into a continuous vector representation, capturing its semantic meaning. The hidden layer is then connected to an output layer, which predicts the context words surrounding the target word."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52e336bb",
   "metadata": {},
   "source": [
    "9. Explain Glove Embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d243feef",
   "metadata": {},
   "source": [
    "ans:GloVe (Global Vectors for Word Representation) is an unsupervised learning algorithm used for generating word embeddings, similar to word2vec. GloVe embeddings capture the semantic and syntactic relationships between words by utilizing global statistics from a large corpus of text.Unlike other word embedding techniques, GloVe does not rely solely on local context (such as neighboring words) or word co-occurrence counts. Instead, it leverages the overall global word co-occurrence statistics in the entire corpus. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "669ba91f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
