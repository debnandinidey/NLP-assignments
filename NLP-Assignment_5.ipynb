{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8fd89be3",
   "metadata": {},
   "source": [
    "#### 1. What are Sequence-to-sequence models?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf20e8de",
   "metadata": {},
   "source": [
    "Sequence-to-sequence (Seq2Seq) models are a type of neural network architecture that is used for tasks involving sequential input and output. These models are designed to take a variable-length input sequence and generate a variable-length output sequence.The basic architecture of a Seq2Seq model consists of two main components: an encoder and a decoder. Seq2Seq models are commonly used for tasks such as machine translation, where the input is a sequence in one language and the output is a sequence in another language. They can also be applied to tasks such as text summarization, speech recognition, and question answering."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c05a3aee",
   "metadata": {},
   "source": [
    "#### 2. What are the Problem with Vanilla RNNs?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c829d49e",
   "metadata": {},
   "source": [
    "Vanilla RNNs (Recurrent Neural Networks) suffer from several problems that limit their effectiveness in capturing long-term dependencies in sequential data:\n",
    "\n",
    "Vanishing Gradient Problem: When training RNNs, gradients can exponentially decrease as they propagate backward through time, leading to difficulties in learning long-term dependencies. This issue occurs due to the repeated multiplication of gradients during backpropagation, causing them to diminish to zero.\n",
    "\n",
    "Exploding Gradient Problem: In contrast to the vanishing gradient problem, gradients can also explode during training, which leads to unstable learning. This occurs when the gradient values become extremely large, causing the network weights to update drastically and impairing convergence.\n",
    "\n",
    "Lack of Long-Term Memory: Vanilla RNNs have limitations in capturing long-term dependencies in sequential data. As the sequences become longer, the models struggle to retain useful information from earlier time steps, resulting in degradation of performance.\n",
    "\n",
    "Difficulty in Capturing Contextual Information: Vanilla RNNs treat each element in the sequence independently without considering the context of neighboring elements. This makes it challenging for the model to capture contextual information and dependencies between distant time steps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da7a5e37",
   "metadata": {},
   "source": [
    "#### 3. What is Gradient clipping?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9658b45",
   "metadata": {},
   "source": [
    "Gradient clipping is a technique used during the training of neural networks, including recurrent neural networks (RNNs), to mitigate the exploding gradient problem. It involves modifying the gradients to ensure they do not exceed a predefined threshold value. The purpose of gradient clipping is to prevent the gradients from becoming too large, which can lead to unstable training and hinder convergence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d2ca953",
   "metadata": {},
   "source": [
    "#### 4. Explain Attention mechanism"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9656891d",
   "metadata": {},
   "source": [
    "Attention mechanism is a technique used in sequence-to-sequence models, particularly in tasks such as machine translation and natural language processing, to selectively focus on specific parts of the input sequence when generating the output sequence. It allows the model to assign different levels of importance or attention to different input elements while making predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf14d07",
   "metadata": {},
   "source": [
    "#### 5. Explain Conditional random fields (CRFs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7090ded",
   "metadata": {},
   "source": [
    "Conditional Random Fields (CRFs) are a probabilistic graphical model used for modeling and predicting sequential data, particularly in tasks such as sequence labeling, named entity recognition, part-of-speech tagging, and semantic role labeling.\n",
    "CRFs are an extension of the Hidden Markov Models (HMMs) and address some of their limitations. While HMMs assume that each observation is conditionally independent of all other observations given the hidden state, CRFs relax this assumption by allowing for more complex dependencies between observations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e309990c",
   "metadata": {},
   "source": [
    "#### 6. Explain self-attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51f853ee",
   "metadata": {},
   "source": [
    "Self-attention, also known as intra-attention or scaled dot-product attention, is a mechanism used in sequence-to-sequence models, particularly in transformer architectures. It allows the model to weigh the importance of different elements within the same input sequence to compute a context-aware representation.The main thing is self-attention is to compute the attention weights between each pair of elements in the input sequence. This is done by comparing the similarity or relevance between the query, key, and value vectors. The query vector represents the current element that is being attended to, while the key and value vectors represent all the other elements in the sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7752b56f",
   "metadata": {},
   "source": [
    "#### 7. What is Bahdanau Attention?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf64cbc5",
   "metadata": {},
   "source": [
    "Bahdanau Attention, also known as additive attention, is an attention mechanism used in sequence-to-sequence models, particularly in neural machine translation. Bahdanau Attention allows the model to focus on different parts of the input sequence while generating each element of the output sequence. It addresses the limitation of traditional encoder-decoder models where a fixed-length context vector is used to summarize the entire input sequence. Bahdanau Attention is to learn context-dependent alignment weights between the input sequence and the output sequence. These alignment weights determine the importance or relevance of each input element at each decoding step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49384983",
   "metadata": {},
   "source": [
    "#### 8. What is a Language Model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b85a8efc",
   "metadata": {},
   "source": [
    "A language model is a statistical model or a computational model that is trained on a large corpus of text to predict the probability of a sequence of words or characters in a given language. It aims to capture the patterns, structures, and statistical properties of a language to generate or predict coherent and meaningful sequences of text.Language models are designed to understand and generate human language. They can be used for various natural language processing tasks, including text generation, machine translation, speech recognition, spell checking, and sentiment analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b80e22fd",
   "metadata": {},
   "source": [
    "#### 9. What is Multi-Head Attention?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f25d8a6e",
   "metadata": {},
   "source": [
    "Multi-Head Attention is a mechanism used in transformer-based models, primarily in the field of natural language processing. It enhances the ability of the model to capture different types of dependencies and relationships between words or tokens in a sequence.attention is a mechanism that assigns weights to different parts of the input sequence, allowing the model to focus on relevant information. Multi-Head Attention extends this mechanism by performing attention operations in parallel, using multiple \"heads\" or sub-attentions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "316c2d44",
   "metadata": {},
   "source": [
    "#### 10. What is Bilingual Evaluation Understudy (BLEU)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3799846",
   "metadata": {},
   "source": [
    "Bilingual Evaluation Understudy (BLEU) is a metric used to evaluate the quality of machine-generated translations by comparing them to human reference translations. It measures the similarity between the machine-generated translation and one or more reference translations.BLEU calculates a precision score by comparing n-grams (contiguous sequences of n words) between the candidate translation and the reference translations. It evaluates the precision of unigrams, bigrams, trigrams, and so on, up to a specified maximum n-gram length. The BLEU score is computed by combining the individual precision scores for different n-grams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66ca568e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
